{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "#### Training/ Fit\n",
    "Given the whole dataset:\n",
    "- Calculate information gain with each possible split\n",
    "- Divide set with that feature and value that gives the most IG \n",
    "- Divide tree and do the same for all created branches\n",
    "- Repeat until a stopping criteria is reached \n",
    "\n",
    " Stopping criteria can be:\n",
    " - maximum depth\n",
    " - minimum number of samples to split\n",
    " - min impurity decrease \n",
    " \n",
    "#### Test\n",
    "Given a data points:\n",
    "- Follow the tree until you reach a leaf node\n",
    "- Return the most common class label - if classification problem\n",
    "\n",
    "#### Test points:\n",
    "- Information gain: IG = entropy of parent - weighted_average * entropy of children\n",
    "- Entropy = - sum of p(x_i) * log(p(x_i)); here p(x_i) = num of obs in class i / total num of obs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import np \n",
    "from collections import Counter\n",
    "class Node:\n",
    "    def __init__(self, feature = None, threshold= None, left = None, right = None, *, value = None):\n",
    "        # specify a tree node\n",
    "        # use the * trick, we ensure user can only update value as keyword arguments.\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.value = value # only leaf node will have value\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None \n",
    "\n",
    "    \n",
    "class DecisionTree:\n",
    "    def __init__(self, min_sample_split = 2 , max_depth = 100, n_features = None):\n",
    "        self.min_samples_split = min_sample_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features # n_feature will less than the total number of features in random forest\n",
    "        self.root = None \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._update_n_features(X.shape[1])\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _update_n_features(self, total_n_features):\n",
    "        # update number of features based on the training data, avoid errors \n",
    "        if not self.n_features:\n",
    "            self.n_features = total_n_features\n",
    "        else:\n",
    "            self.n_features = min(total_n_features, self.n_features)\n",
    "\n",
    "\n",
    "    def _grow_tree(self, X, y):\n",
    "        # grow the tree recursively \n",
    "        # 0. get information abuot the current subset of data\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "        # 1. check the stopping criteria \n",
    "        if depth >= self.max_depth or n_labels == 1 or n_samples  <= self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value = )\n",
    "        # 2. find the best split \n",
    "        # 3. create child nodes \n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        pass \n",
    "    def predict():\n",
    "        pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
